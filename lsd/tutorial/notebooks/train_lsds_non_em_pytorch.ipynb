{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title install packages + repos\n",
        "\n",
        "!pip install albumentations\n",
        "!pip install gunpowder\n",
        "!pip install git+https://github.com/funkelab/funlib.learn.torch.git\n",
        "!pip install git+https://github.com/funkelab/lsd.git"
      ],
      "metadata": {
        "id": "CGd6dGCpmETE",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title import packages\n",
        "\n",
        "import albumentations as A\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "from funlib.learn.torch.models import UNet, ConvPass\n",
        "from glob import glob\n",
        "from lsd.train import local_shape_descriptor\n",
        "from scipy.ndimage import binary_erosion\n",
        "from skimage.measure import label\n",
        "from skimage.io import imread\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "EN3ThhL8gMGb",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "ffo-dVdeU-ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download microscopy benchmark data (https://bbbc.broadinstitute.org/)\n",
        "\n",
        "!wget 'https://data.broadinstitute.org/bbbc/BBBC039/images.zip'\n",
        "!unzip images.zip\n",
        "!rm images.zip\n",
        "\n",
        "!wget 'https://data.broadinstitute.org/bbbc/BBBC039/masks.zip'\n",
        "!unzip masks.zip\n",
        "!rm masks.zip"
      ],
      "metadata": {
        "id": "ipM8xWJFgZZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title utility function to view labels\n",
        "\n",
        "# matplotlib uses a default shader\n",
        "# we need to recolor as unique objects\n",
        "\n",
        "def create_lut(labels):\n",
        "\n",
        "    max_label = np.max(labels)\n",
        "\n",
        "    lut = np.random.randint(\n",
        "            low=0,\n",
        "            high=255,\n",
        "            size=(int(max_label + 1), 3),\n",
        "            dtype=np.uint8)\n",
        "\n",
        "    lut = np.append(\n",
        "            lut,\n",
        "            np.zeros(\n",
        "                (int(max_label + 1), 1),\n",
        "                dtype=np.uint8) + 255,\n",
        "            axis=1)\n",
        "\n",
        "    lut[0] = 0\n",
        "    colored_labels = lut[labels]\n",
        "\n",
        "    return colored_labels"
      ],
      "metadata": {
        "id": "BRyRTZk6i_c7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view a sample image\n",
        "\n",
        "image_files = sorted(glob('images/*.tif'))\n",
        "mask_files = sorted(glob('masks/*.png'))\n",
        "\n",
        "test_image = imread(image_files[0])\n",
        "test_mask = imread(mask_files[0])[:,:,0]\n",
        "\n",
        "plt.imshow(test_image, cmap='gray')\n",
        "plt.imshow(create_lut(label(test_mask)), alpha=0.5)"
      ],
      "metadata": {
        "id": "3kIVYvanh3Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example augmentations\n",
        "\n",
        "file = random.choice(image_files)\n",
        "\n",
        "test_image = imread(file)\n",
        "test_mask = imread(\n",
        "    file.replace('images', 'masks').replace('.tif', '.png')\n",
        ")\n",
        "\n",
        "test_mask = label(test_mask[:,:,0])\n",
        "\n",
        "transform = A.Compose([\n",
        "              A.RandomCrop(width=64, height=64),\n",
        "              A.HorizontalFlip(p=0.5),\n",
        "              A.VerticalFlip(p=0.5)\n",
        "            ])\n",
        "\n",
        "transformed = transform(image=test_image, mask=test_mask)\n",
        "          \n",
        "aug_raw, aug_mask = transformed['image'], transformed['mask']\n",
        "\n",
        "fig, axes = plt.subplots(1,2,figsize=(10, 10),sharex=False,sharey=False,squeeze=False)\n",
        "\n",
        "axes[0][0].imshow(test_image, cmap='gray')\n",
        "axes[0][0].imshow(create_lut(test_mask), alpha=0.5)\n",
        "axes[0][0].set_title('original image')\n",
        "\n",
        "axes[0][1].imshow(aug_raw, cmap='gray')\n",
        "axes[0][1].imshow(create_lut(aug_mask), alpha=0.5)\n",
        "axes[0][1].set_title('example random augmentation')"
      ],
      "metadata": {
        "id": "W_ays8YimOKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlKylm3Cf0A-"
      },
      "outputs": [],
      "source": [
        "# create a torch dataset\n",
        "\n",
        "class CellDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_dir,\n",
        "        mask_dir,\n",
        "        split='train',\n",
        "        crop_size=None,\n",
        "        padding_size=8):\n",
        "\n",
        "      self.images = sorted(glob(f'{image_dir}/*.tif'))\n",
        "      self.masks = sorted(glob(f'{mask_dir}/*.png'))\n",
        "\n",
        "      self.split = split\n",
        "      self.crop_size = crop_size\n",
        "      self.padding_size = padding_size\n",
        "\n",
        "      if split == 'test':\n",
        "        self.images = self.images[:10]\n",
        "        self.masks = self.masks[:10]\n",
        "      elif split == 'val':\n",
        "        self.images = self.images[10:20]\n",
        "        self.masks = self.masks[10:20]\n",
        "      else:\n",
        "        self.images = self.images[20:]\n",
        "        self.masks = self.masks[20:]\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.images)\n",
        "\n",
        "    # function to erode label boundaries\n",
        "    def erode(self, labels, iterations, border_value):\n",
        "\n",
        "      foreground = np.zeros_like(labels, dtype=bool)\n",
        "\n",
        "      # loop through unique labels\n",
        "      for label in np.unique(labels):\n",
        "\n",
        "          # skip background\n",
        "          if label == 0:\n",
        "              continue\n",
        "\n",
        "          # mask to label\n",
        "          label_mask = labels == label\n",
        "\n",
        "          # erode labels\n",
        "          eroded_mask = binary_erosion(\n",
        "                  label_mask,\n",
        "                  iterations=iterations,\n",
        "                  border_value=border_value)\n",
        "\n",
        "          # get foreground\n",
        "          foreground = np.logical_or(eroded_mask, foreground)\n",
        "\n",
        "      # and background...\n",
        "      background = np.logical_not(foreground)\n",
        "\n",
        "      # set eroded pixels to zero\n",
        "      labels[background] = 0\n",
        "\n",
        "      return labels\n",
        "\n",
        "    # takes care of padding\n",
        "    def get_padding(self, crop_size, padding_size):\n",
        "    \n",
        "        # quotient\n",
        "        q = int(crop_size / padding_size)\n",
        "    \n",
        "        if crop_size % padding_size != 0:\n",
        "            padding = (padding_size * (q + 1))\n",
        "        else:\n",
        "            padding = crop_size\n",
        "    \n",
        "        return padding\n",
        "    \n",
        "    # sample augmentations (see https://albumentations.ai/docs/examples/example_kaggle_salt)\n",
        "    def augment_data(self, raw, mask, padding):\n",
        "        \n",
        "        transform = A.Compose([\n",
        "              A.RandomCrop(\n",
        "                  width=self.crop_size,\n",
        "                  height=self.crop_size),\n",
        "              A.PadIfNeeded(\n",
        "                  min_height=padding,\n",
        "                  min_width=padding,\n",
        "                  p=1,\n",
        "                  border_mode=0),\n",
        "              A.HorizontalFlip(p=0.3),\n",
        "              A.VerticalFlip(p=0.3),\n",
        "              A.RandomRotate90(p=0.3),\n",
        "              A.Transpose(p=0.3),\n",
        "              A.RandomBrightnessContrast(p=0.3)\n",
        "            ])\n",
        "\n",
        "        transformed = transform(image=raw, mask=mask)\n",
        "\n",
        "        raw, mask = transformed['image'], transformed['mask']\n",
        "        \n",
        "        return raw, mask\n",
        "\n",
        "    # normalize raw data between 0 and 1\n",
        "    def normalize(self, data):\n",
        "      return (data - np.min(data)) / (np.max(data) - np.min(data)).astype(np.float32)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "      raw = self.images[idx]\n",
        "      labels = self.masks[idx]\n",
        "\n",
        "      raw = imread(raw)\n",
        "      raw = self.normalize(raw)\n",
        "      \n",
        "      # slice first channel, relabel connected components\n",
        "      labels = label(imread(labels)[:,:,0]).astype(np.uint16)\n",
        "\n",
        "      padding = self.get_padding(self.crop_size, self.padding_size)\n",
        "      raw, labels = self.augment_data(raw, labels, padding)\n",
        "\n",
        "      if self.split != 'test':\n",
        "\n",
        "        # if train/val, generate our gt labels\n",
        "\n",
        "        labels = self.erode(\n",
        "            labels,\n",
        "            iterations=1,\n",
        "            border_value=1)\n",
        "        \n",
        "        lsds = local_shape_descriptor.get_local_shape_descriptors(\n",
        "                segmentation=labels,\n",
        "                sigma=(5,)*2,\n",
        "                voxel_size=(1,)*2)\n",
        "\n",
        "        raw = np.expand_dims(raw, axis=0)\n",
        "        lsds = lsds.astype(np.float32)\n",
        "    \n",
        "        return raw, lsds\n",
        "      \n",
        "      else:\n",
        "        return np.expand_dims(raw, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# view example batch\n",
        "\n",
        "train_dataset = CellDataset(\n",
        "    image_dir='images',\n",
        "    mask_dir='masks',\n",
        "    split='train',\n",
        "    crop_size=128)\n",
        "\n",
        "raw, lsds = next(iter(train_dataset))\n",
        "\n",
        "fig, axes = plt.subplots(\n",
        "            1,\n",
        "            2,\n",
        "            figsize=(20, 6),\n",
        "            sharex=True,\n",
        "            sharey=True,\n",
        "            squeeze=False)\n",
        "\n",
        "axes[0][0].imshow(np.squeeze(raw), cmap='gray')\n",
        "axes[0][1].imshow(lsds[0:3].transpose((1,2,0)), )"
      ],
      "metadata": {
        "id": "-9JmJHJnj0ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title simplified unet\n",
        "#modified from https://github.com/dlmbl/instance_segmentation/blob/main/unet.py\n",
        "\n",
        "class ConvPass(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_sizes, padding, activation):\n",
        "\n",
        "        super(ConvPass, self).__init__()\n",
        "\n",
        "        if activation is not None:\n",
        "            activation = getattr(torch.nn, activation)\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        for kernel_size in kernel_sizes:\n",
        "            self.dims = len(kernel_size)\n",
        "            if padding in (\"VALID\", \"valid\"):\n",
        "                pad = 0\n",
        "            elif padding in (\"SAME\", \"same\"):\n",
        "                pad = tuple(np.array(kernel_size) // 2)\n",
        "            else:\n",
        "                raise RuntimeError(\"invalid string value for padding\")\n",
        "            layers.append(\n",
        "                torch.nn.Conv2d(in_channels, out_channels, kernel_size, padding=pad)\n",
        "            )\n",
        "            in_channels = out_channels\n",
        "\n",
        "            if activation is not None:\n",
        "                layers.append(activation())\n",
        "\n",
        "        self.conv_pass = torch.nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv_pass(x)\n",
        "\n",
        "\n",
        "class Downsample(torch.nn.Module):\n",
        "    def __init__(self, downsample_factor):\n",
        "\n",
        "        super(Downsample, self).__init__()\n",
        "\n",
        "        self.dims = len(downsample_factor)\n",
        "        self.downsample_factor = downsample_factor\n",
        "\n",
        "        self.down = torch.nn.MaxPool2d(downsample_factor, stride=downsample_factor)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for d in range(1, self.dims + 1):\n",
        "            if x.size()[-d] % self.downsample_factor[-d] != 0:\n",
        "                raise RuntimeError(\n",
        "                    \"Can not downsample shape %s with factor %s, mismatch \"\n",
        "                    \"in spatial dimension %d\"\n",
        "                    % (x.size(), self.downsample_factor, self.dims - d)\n",
        "                )\n",
        "\n",
        "        return self.down(x)\n",
        "\n",
        "\n",
        "class Upsample(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        scale_factor,\n",
        "        mode=\"nearest\",\n",
        "        in_channels=None,\n",
        "        out_channels=None,\n",
        "        crop_factor=None,\n",
        "        padding=\"VALID\",\n",
        "        next_conv_kernel_sizes=None,\n",
        "    ):\n",
        "\n",
        "        super(Upsample, self).__init__()\n",
        "\n",
        "        assert (crop_factor is None) == (\n",
        "            next_conv_kernel_sizes is None\n",
        "        ), \"crop_factor and next_conv_kernel_sizes have to be given together\"\n",
        "\n",
        "        self.crop_factor = crop_factor\n",
        "        self.next_conv_kernel_sizes = next_conv_kernel_sizes\n",
        "        self.padding = padding\n",
        "\n",
        "        self.dims = len(scale_factor)\n",
        "\n",
        "        if mode == \"transposed_conv\":\n",
        "            self.up = torch.nn.ConvTranspose2d(\n",
        "                in_channels, out_channels, kernel_size=scale_factor, stride=scale_factor\n",
        "            )\n",
        "        else:\n",
        "            self.up = torch.nn.Upsample(scale_factor=tuple(scale_factor), mode=mode)\n",
        "\n",
        "    def crop_to_factor(self, x, factor, kernel_sizes):\n",
        "        \"\"\"Crop feature maps to ensure translation equivariance with stride of\n",
        "        upsampling factor. This should be done right after upsampling, before\n",
        "        application of the convolutions with the given kernel sizes.\n",
        "        The crop could be done after the convolutions, but it is more efficient\n",
        "        to do that before (feature maps will be smaller).\n",
        "        \"\"\"\n",
        "\n",
        "        shape = x.size()\n",
        "        spatial_shape = shape[-self.dims :]\n",
        "\n",
        "        # the crop that will already be done due to the convolutions\n",
        "        convolution_crop = tuple(\n",
        "            sum(ks[d] - 1 for ks in kernel_sizes) for d in range(self.dims)\n",
        "        )\n",
        "\n",
        "        # we need (spatial_shape - convolution_crop) to be a multiple of\n",
        "        # factor, i.e.:\n",
        "        #\n",
        "        # (s - c) = n*k\n",
        "        #\n",
        "        # we want to find the largest n for which s' = n*k + c <= s\n",
        "        #\n",
        "        # n = floor((s - c)/k)\n",
        "        #\n",
        "        # this gives us the target shape s'\n",
        "        #\n",
        "        # s' = n*k + c\n",
        "\n",
        "        ns = (\n",
        "            int(math.floor(float(s - c) / f))\n",
        "            for s, c, f in zip(spatial_shape, convolution_crop, factor)\n",
        "        )\n",
        "        target_spatial_shape = tuple(\n",
        "            n * f + c for n, c, f in zip(ns, convolution_crop, factor)\n",
        "        )\n",
        "\n",
        "        if target_spatial_shape != spatial_shape:\n",
        "\n",
        "            assert all(\n",
        "                ((t > c) for t, c in zip(target_spatial_shape, convolution_crop))\n",
        "            ), (\n",
        "                \"Feature map with shape %s is too small to ensure \"\n",
        "                \"translation equivariance with factor %s and following \"\n",
        "                \"convolutions %s\" % (shape, factor, kernel_sizes)\n",
        "            )\n",
        "\n",
        "            return self.crop(x, target_spatial_shape)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def crop(self, x, shape):\n",
        "        \"\"\"Center-crop x to match spatial dimensions given by shape.\"\"\"\n",
        "\n",
        "        x_target_size = x.size()[: -self.dims] + shape\n",
        "\n",
        "        offset = tuple((a - b) // 2 for a, b in zip(x.size(), x_target_size))\n",
        "\n",
        "        slices = tuple(slice(o, o + s) for o, s in zip(offset, x_target_size))\n",
        "\n",
        "        return x[slices]\n",
        "\n",
        "    def forward(self, f_left, g_out):\n",
        "\n",
        "        g_up = self.up(g_out)\n",
        "\n",
        "        # if self.next_conv_kernel_sizes is not None and self.padding in (\"VALID\", \"valid\"):\n",
        "        #    g_cropped = self.crop_to_factor(\n",
        "        #        g_up,\n",
        "        #        self.crop_factor,\n",
        "        #        self.next_conv_kernel_sizes)\n",
        "        # else:\n",
        "        g_cropped = g_up\n",
        "\n",
        "        f_cropped = self.crop(f_left, g_cropped.size()[-self.dims :])\n",
        "\n",
        "        return torch.cat([f_cropped, g_cropped], dim=1)\n",
        "\n",
        "\n",
        "class UNet(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        num_fmaps,\n",
        "        fmap_inc_factors,\n",
        "        downsample_factors,\n",
        "        kernel_size_down=None,\n",
        "        kernel_size_up=None,\n",
        "        activation=\"ReLU\",\n",
        "        padding=\"VALID\",\n",
        "        num_fmaps_out=None,\n",
        "        constant_upsample=False,\n",
        "    ):\n",
        "        \"\"\"Create a U-Net::\n",
        "            f_in --> f_left --------------------------->> f_right--> f_out\n",
        "                        |                                   ^\n",
        "                        v                                   |\n",
        "                     g_in --> g_left ------->> g_right --> g_out\n",
        "                                 |               ^\n",
        "                                 v               |\n",
        "                                       ...\n",
        "        where each ``-->`` is a convolution pass, each `-->>` a crop, and down\n",
        "        and up arrows are max-pooling and transposed convolutions,\n",
        "        respectively.\n",
        "        The U-Net expects 2D tensors shaped like::\n",
        "            ``(batch=1, channels, height, width)``.\n",
        "        This U-Net performs only \"valid\" convolutions, i.e., sizes of the\n",
        "        feature maps decrease after each convolution.\n",
        "        Args:\n",
        "            in_channels:\n",
        "                The number of input channels.\n",
        "            num_fmaps:\n",
        "                The number of feature maps in the first layer. This is also the\n",
        "                number of output feature maps. Stored in the ``channels``\n",
        "                dimension.\n",
        "            fmap_inc_factors:\n",
        "                By how much to multiply the number of feature maps between\n",
        "                layers. If layer 0 has ``k`` feature maps, layer ``l`` will\n",
        "                have ``k*fmap_inc_factor**l``.\n",
        "            downsample_factors:\n",
        "                List of tuples ``(y, x)`` to use to down- and up-sample the\n",
        "                feature maps between layers.\n",
        "            kernel_size_down (optional):\n",
        "                List of lists of kernel sizes. The number of sizes in a list\n",
        "                determines the number of convolutional layers in the\n",
        "                corresponding level of the build on the left side. Kernel sizes\n",
        "                can be given as tuples or integer. If not given, each\n",
        "                convolutional pass will consist of two 3x3 convolutions.\n",
        "            kernel_size_up (optional):\n",
        "                List of lists of kernel sizes. The number of sizes in a list\n",
        "                determines the number of convolutional layers in the\n",
        "                corresponding level of the build on the right side. Within one\n",
        "                of the lists going from left to right. Kernel sizes can be\n",
        "                given as tuples or integer. If not given, each convolutional\n",
        "                pass will consist of two 3x3 convolutions.\n",
        "            activation:\n",
        "                Which activation to use after a convolution. Accepts the name\n",
        "                of any tensorflow activation function (e.g., ``ReLU`` for\n",
        "                ``torch.nn.ReLU``).\n",
        "            fov (optional):\n",
        "                Initial field of view\n",
        "            constant_upsample (optional):\n",
        "                If set to true, perform a constant upsampling instead of a\n",
        "                transposed convolution in the upsampling layers.\n",
        "            padding (optional):\n",
        "                How to pad convolutions. Either 'same' or 'valid' (default).\n",
        "        \"\"\"\n",
        "\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        self.num_levels = len(downsample_factors) + 1\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = num_fmaps_out if num_fmaps_out else num_fmaps\n",
        "        self.constant_upsample = constant_upsample\n",
        "\n",
        "        # default arguments\n",
        "        if kernel_size_down is None:\n",
        "            kernel_size_down = [[(3, 3), (3, 3)]] * self.num_levels\n",
        "        if kernel_size_up is None:\n",
        "            kernel_size_up = [[(3, 3), (3, 3)]] * (self.num_levels - 1)\n",
        "\n",
        "        self.kernel_size_down = kernel_size_down\n",
        "        self.kernel_size_up = kernel_size_up\n",
        "        self.downsample_factors = downsample_factors\n",
        "\n",
        "        # compute crop factors for translation equivariance\n",
        "        crop_factors = []\n",
        "        factor_product = None\n",
        "        for factor in downsample_factors:\n",
        "            if factor_product is None:\n",
        "                factor_product = list(factor)\n",
        "            else:\n",
        "                factor_product = list(f * ff for f, ff in zip(factor, factor_product))\n",
        "            crop_factors.append(factor_product)\n",
        "        crop_factors = crop_factors[::-1]\n",
        "\n",
        "        # modules\n",
        "\n",
        "        # left convolutional passes\n",
        "        self.l_conv = nn.ModuleList(\n",
        "            [\n",
        "                ConvPass(\n",
        "                    in_channels\n",
        "                    if level == 0\n",
        "                    else num_fmaps * fmap_inc_factors ** (level - 1),\n",
        "                    num_fmaps * fmap_inc_factors**level,\n",
        "                    kernel_size_down[level],\n",
        "                    padding,\n",
        "                    activation=activation,\n",
        "                )\n",
        "                for level in range(self.num_levels)\n",
        "            ]\n",
        "        )\n",
        "        self.dims = self.l_conv[0].dims\n",
        "\n",
        "        # left downsample layers\n",
        "        self.l_down = nn.ModuleList(\n",
        "            [\n",
        "                Downsample(downsample_factors[level])\n",
        "                for level in range(self.num_levels - 1)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # right up/crop/concatenate layers\n",
        "        self.r_up = nn.ModuleList(\n",
        "            [\n",
        "                Upsample(\n",
        "                    downsample_factors[level],\n",
        "                    mode=\"nearest\" if constant_upsample else \"transposed_conv\",\n",
        "                    in_channels=num_fmaps * fmap_inc_factors ** (level + 1),\n",
        "                    out_channels=num_fmaps * fmap_inc_factors ** (level + 1),\n",
        "                    crop_factor=crop_factors[level],\n",
        "                    padding=padding,\n",
        "                    next_conv_kernel_sizes=kernel_size_up[level],\n",
        "                )\n",
        "                for level in range(self.num_levels - 1)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # right convolutional passes\n",
        "        self.r_conv = nn.ModuleList(\n",
        "            [\n",
        "                ConvPass(\n",
        "                    num_fmaps * fmap_inc_factors**level\n",
        "                    + num_fmaps * fmap_inc_factors ** (level + 1),\n",
        "                    num_fmaps * fmap_inc_factors**level\n",
        "                    if num_fmaps_out is None or level != 0\n",
        "                    else num_fmaps_out,\n",
        "                    kernel_size_up[level],\n",
        "                    padding,\n",
        "                    activation=activation,\n",
        "                )\n",
        "                for level in range(self.num_levels - 1)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def rec_fov(self, level, fov, sp):\n",
        "\n",
        "        # index of level in layer arrays\n",
        "        i = self.num_levels - level - 1\n",
        "\n",
        "        # convolve\n",
        "        for j in range(len(self.kernel_size_down[i])):\n",
        "            fov += (np.array(self.kernel_size_down[i][j]) - 1) * sp\n",
        "\n",
        "        # end of recursion\n",
        "        if level != 0:\n",
        "            # down\n",
        "            fov += (np.array(self.downsample_factors[i]) - 1) * sp\n",
        "            sp *= np.array(self.downsample_factors[i])\n",
        "\n",
        "            # nested levels\n",
        "            fov, sp = self.rec_fov(level - 1, fov, sp)\n",
        "\n",
        "            # up\n",
        "            sp //= np.array(self.downsample_factors[i])\n",
        "\n",
        "            # convolve\n",
        "            for j in range(len(self.kernel_size_up[i])):\n",
        "                fov += (np.array(self.kernel_size_up[i][j]) - 1) * sp\n",
        "\n",
        "        return fov, sp\n",
        "\n",
        "    def get_fov(self):\n",
        "        fov, sp = self.rec_fov(self.num_levels - 1, (1, 1), 1)\n",
        "        return fov\n",
        "\n",
        "    def rec_forward(self, level, f_in):\n",
        "\n",
        "        # index of level in layer arrays\n",
        "        i = self.num_levels - level - 1\n",
        "\n",
        "        # convolve\n",
        "        f_left = self.l_conv[i](f_in)\n",
        "\n",
        "        # end of recursion\n",
        "        if level == 0:\n",
        "            fs_out = f_left\n",
        "        else:\n",
        "            # down\n",
        "            g_in = self.l_down[i](f_left)\n",
        "            # nested levels\n",
        "            gs_out = self.rec_forward(level - 1, g_in)\n",
        "            # up, concat, and crop\n",
        "            fs_right = self.r_up[i](f_left, gs_out)\n",
        "\n",
        "            # convolve\n",
        "            fs_out = self.r_conv[i](fs_right)\n",
        "\n",
        "        return fs_out\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        y = self.rec_forward(self.num_levels - 1, x)\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "60RvAAYv27lp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# create our network, 1 input channels in the raw data\n",
        "\n",
        "d_factors = [[2,2],[2,2],[2,2]]\n",
        "\n",
        "in_channels=1\n",
        "num_fmaps=12\n",
        "fmap_inc_factor=5\n",
        "\n",
        "unet = UNet(\n",
        "    in_channels=in_channels,\n",
        "    num_fmaps=num_fmaps,\n",
        "    fmap_inc_factors=fmap_inc_factor,\n",
        "    downsample_factors=d_factors,\n",
        "    padding='same',\n",
        "    constant_upsample=True)\n",
        "\n",
        "model = torch.nn.Sequential(\n",
        "    unet,\n",
        "    torch.nn.Conv2d(in_channels=num_fmaps,out_channels=6, kernel_size=1)\n",
        ").to(device)\n",
        "\n",
        "loss_fn = torch.nn.MSELoss().to(device)"
      ],
      "metadata": {
        "id": "_7U3zH0DsjBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_steps = 1000\n",
        "\n",
        "# set optimizer\n",
        "learning_rate = 1e-4\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# set activation\n",
        "activation = torch.nn.Sigmoid()\n",
        "\n",
        "### create datasets\n",
        "\n",
        "train_dataset = CellDataset(image_dir='images', mask_dir='masks', split='train', crop_size=128)\n",
        "val_dataset = CellDataset(image_dir='images', mask_dir='masks', split='val', crop_size=128)\n",
        "test_dataset = CellDataset(image_dir='images', mask_dir='masks', split='test', crop_size=256)\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "# make dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset)\n",
        "val_loader = DataLoader(val_dataset)"
      ],
      "metadata": {
        "id": "6JIuzJ9-snse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training step\n",
        "\n",
        "def model_step(model, loss_fn, optimizer, feature, gt_lsds, activation, train_step=True):\n",
        "    \n",
        "    # zero gradients if training\n",
        "    if train_step:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "    # forward\n",
        "    lsd_logits = model(feature)\n",
        "\n",
        "    loss_value = loss_fn(lsd_logits, gt_lsds)\n",
        "    \n",
        "    # backward if training mode\n",
        "    if train_step:\n",
        "        loss_value.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    lsd_output = activation(lsd_logits)\n",
        "   \n",
        "    outputs = {\n",
        "        'pred_lsds': lsd_output,\n",
        "        'lsds_logits': lsd_logits,\n",
        "    }\n",
        "    \n",
        "    return loss_value, outputs"
      ],
      "metadata": {
        "id": "2cV7ssA4s7x8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "\n",
        "# set flags\n",
        "model.train() \n",
        "loss_fn.train()\n",
        "step = 0\n",
        "\n",
        "with tqdm(total=training_steps) as pbar:\n",
        "    while step < training_steps:\n",
        "        # reset data loader to get random augmentations\n",
        "        np.random.seed()\n",
        "        tmp_loader = iter(train_loader)\n",
        "\n",
        "        for feature, gt_lsds in tmp_loader:\n",
        "            feature = feature.to(device)\n",
        "            gt_lsds = gt_lsds.to(device)\n",
        "                                          \n",
        "            loss_value, pred = model_step(model, loss_fn, optimizer, feature, gt_lsds, activation)\n",
        "            step += 1\n",
        "            pbar.update(1)\n",
        "            \n",
        "            if step % 100 == 0:\n",
        "                model.eval()\n",
        "                tmp_val_loader = iter(val_loader)\n",
        "                acc_loss = []\n",
        "                for feature, gt_lsds in tmp_val_loader:\n",
        "                    feature = feature.to(device)\n",
        "                    gt_lsds = gt_lsds.to(device)\n",
        "\n",
        "                    loss_value, _ = model_step(model, loss_fn, optimizer, feature, gt_lsds, activation, train_step=False)\n",
        "                    acc_loss.append(loss_value.cpu().detach().numpy())\n",
        "                model.train()\n",
        "                print(np.mean(acc_loss))"
      ],
      "metadata": {
        "id": "s4KKiBtWuSrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize a few predictions \n",
        "\n",
        "model.eval()\n",
        "\n",
        "activation = torch.nn.Sigmoid()\n",
        "\n",
        "for idx, image in enumerate(test_loader):\n",
        "    image = image.to(device)\n",
        "\n",
        "    lsds_logits = model(image)\n",
        "    pred_lsds = activation(lsds_logits)\n",
        "        \n",
        "    image = np.squeeze(image.cpu())\n",
        "    \n",
        "    pred_lsds = np.squeeze(pred_lsds.cpu().detach().numpy())\n",
        "    \n",
        "    fig, axes = plt.subplots(1,2,figsize=(20, 20),sharex=True,sharey=True,squeeze=False)\n",
        "    \n",
        "    axes[0][0].imshow(image, cmap='gray')\n",
        "    axes[0][0].title.set_text('Raw')\n",
        "  \n",
        "    axes[0][1].imshow(np.squeeze(pred_lsds[0]), cmap='jet')\n",
        "    axes[0][1].imshow(np.squeeze(pred_lsds[1]), cmap='jet', alpha=0.5)\n",
        "    axes[0][1].title.set_text('Mean offsets')"
      ],
      "metadata": {
        "id": "Xh0sCthR3Saw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}